# Text Tug of War: Unmasking the Machines with ML

## The Problem:

The goal of this project is to create a machine learning model capable of distinguishing between texts generated by humans and AI from a mixed corpus. We train our models using two separate datasets from different domains. These datasets are structured as dictionaries, containing two keys: 'text', representing word sequences mapped to indices from 0 to 4999, and 'label' indicating ‘0’ for AI-generated data and ‘1’ for human-generated. Domain 1 dataset consists of 19500 samples produced by humans and a single AI model, with equal class distribution. Domain 2 dataset, on the other hand, comprises 14500 samples having substantial imbalance, with 2150 human-generated and 12750 machine-generated texts, produced by 7 AI models different from the one in domain 1. The test dataset includes texts from both domains, evenly split with unknown labels. Our primary challenge is to construct a model that can efficiently handle samples from distinct domains and adapt to the challenge of imbalanced data distribution.

## File Description:

- <kbd>/data</kbd>: contains the training and test dataset
- <kbd>LightGBM Model.ipynb</kbd>: final LGBM model
- <kbd>SVM.ipynb</kbd>: final SVM model
- <kbd>Mixed Model.ipynb</kbd>: final ensemble model
- <kbd>/Outputs</kbd>: contains the corresponding outputs of the models

## Main Approach:

- Stratified Undersampling: 
The approach involved addressing class imbalance in the dataset, specifically in domain 2, where machine-generated and human-generated text had an unequal distribution. Stratified undersampling was used to create a balanced representation of both classes while preserving the proportion of text generated by each of the 7 models, resulting in 2150 instances each. This approach prevented overemphasis on dominant classes, though it reduced the dataset size and could potentially impact model generalization.

- LGBM and SVM:
The approach involved using two machine learning models: Light Gradient Boosting Machine (LGBM) and Support Vector Machine (SVM). LGBM employed ensemble learning with gradient-based optimization and histogram-based learning for efficient feature handling. SVM used hyperplane separation in a high-dimensional space with Principal Component Analysis (PCA) for dimensionality reduction. The models underwent a 3-step training process on different domains and were optimized using Grid Search. Both models achieved high test accuracies, with SVM scoring 0.796 and LGBM reaching 0.800.

- Confidence-Weighted Ensemble Model:
The approach involved creating a mixed model by combining SVM and LGBM classifiers to improve predictive accuracy and robustness. This ensemble strategy used predicted probabilities from both models and a confidence-based decision logic. If either model had high confidence, the corresponding label was assigned; otherwise, an averaging mechanism was used. This approach achieved a test accuracy of 0.814, combining SVM's discrimination power with LGBM's flexibility while reducing the risk of overfitting. However, it increased computational complexity and may require calibration of probability thresholds in different contexts.

## Performance:

| Model | Training Time | Train Acc | Test Acc |
|-------|---------------|-----------|----------|
| LGBM	| ~15 sec	| 0.871	    | 0.800    |
| SVM	| ~8 mins	| 0.901	    | 0.796    |	
| Ensemble	| -		| -	    | 0.814    |

## Conclusion:

In conclusion, our project addressed the intricate challenge of text generation detection with a comprehensive approach. Leveraging a carefully designed ensemble of SVM and LGBM models, coupled with strategic data preprocessing, we achieved notable success in enhancing accuracy and confidence in the classification of human-generated and machine-generated text. This project demonstrates the potential of such intricate techniques in tackling complex real-world text classification problems.
